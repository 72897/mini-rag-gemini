# Mini RAG Application (Gemini + Streamlit)

A production-style **Retrieval-Augmented Generation (RAG)** system that allows users to upload documents and ask context-aware questions using Google's Gemini LLM. The application performs semantic search over uploaded documents and generates grounded answers with citations and chat memory support.

---

## ğŸš€ Features

* PDF and TXT document upload
* Automatic text chunking
* Sentence Transformer embeddings
* FAISS vector database for semantic search
* Gemini 3 Flash LLM integration
* Source-based answers with citations
* Multi-turn chat history (conversation memory)
* Streamlit interactive web interface

---

## ğŸ›  Tech Stack

* **Frontend UI:** Streamlit
* **LLM:** Google Gemini 3 Flash
* **Embeddings:** Sentence Transformers
* **Vector Database:** FAISS
* **Framework:** LangChain
* **Programming Language:** Python

---

## ğŸ“ Project Structure

```
mini-rag-gemini
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ data/
â”œâ”€â”€ vector_store/
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py
â”‚   â”œâ”€â”€ chunker.py
â”‚   â”œâ”€â”€ embedder.py
â”‚   â””â”€â”€ vector_store.py
â””â”€â”€ README.md
```

---

## âš™ Installation & Setup

### 1ï¸âƒ£ Clone Repository

```bash
git clone <your-github-repo-url>
cd mini-rag-gemini
```

---

### 2ï¸âƒ£ Create Virtual Environment

#### Windows:

```bash
python -m venv venv
venv\Scripts\activate
```

#### Mac/Linux:

```bash
python3 -m venv venv
source venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ” Environment Configuration

Create a `.env` file in root directory:

```
GEMINI_API_KEY=your_api_key_here
```

âš  Important:

* Do NOT push `.env` file to GitHub
* Keep API key private

---

## â–¶ Run Application

Start Streamlit server:

```bash
streamlit run app.py
```

Application will be available at:

```
http://localhost:8501
```

---

## ğŸ“Š How The System Works

1. User uploads PDF or TXT document
2. Document is split into chunks
3. Embeddings are generated using Sentence Transformers
4. Vectors stored in FAISS database
5. User query triggers semantic similarity search
6. Retrieved chunks + chat history sent to Gemini
7. Gemini generates grounded response with citations

---

## ğŸ§  RAG Pipeline Architecture

```
User Query
     â†“
Chat History Memory
     â†“
Vector Similarity Search (FAISS)
     â†“
Relevant Context Retrieval
     â†“
Prompt Injection
     â†“
Gemini LLM
     â†“
Final Answer + Citations
```

---

## ğŸ¯ Use Cases

* Enterprise document Q&A
* Research assistants
* Knowledge base chatbots
* Customer support automation
* Internal document search tools

---

## ğŸ“Œ Key Highlights

* Prevents hallucinations using context grounding
* Supports follow-up questions via conversation memory
* Lightweight local vector storage
* Easy cloud deployment
* Internship assessment ready

---

## ğŸ‘¨â€ğŸ’» Author

**Kunal Singh**
Track B â€” AI Engineer Internship Assessment

---

## â­ Future Improvements (Optional)

* Hybrid search (keyword + vector)
* Reranking with Cross Encoders
* Multi-file indexing
* Streaming LLM responses
* Authentication layer
* Cloud vector database integration
